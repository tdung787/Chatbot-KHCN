import os
import json
import re
import logging
from pathlib import Path
from typing import List, Dict
from datetime import datetime
from tqdm import tqdm

# ================== CONFIG ==================
INPUT_FOLDER = "data/output/assigned_answers"
OUTPUT_JSON = "database/parsed_questions.json"
STATS_JSON = "database/parsing_statistics.json"

# ================== LOGGING SETUP ==================
def setup_logging():
    """Setup logging"""
    log_folder = Path("database/logs")
    log_folder.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_folder / f"parsing_{timestamp}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# ================== PARSE QUESTION FILE ==================
def normalize_text(text: str) -> str:
    """
    Chu·∫©n h√≥a text: lo·∫°i b·ªè xu·ªëng d√≤ng th·ª´a, kho·∫£ng tr·∫Øng th·ª´a
    """
    # Thay \n b·∫±ng space
    text = text.replace('\n', ' ')
    # Lo·∫°i b·ªè nhi·ªÅu space li√™n ti·∫øp
    text = re.sub(r'\s+', ' ', text)
    # Trim
    return text.strip()

def parse_all_questions(content: str, page_markers: List[tuple]) -> List[Dict]:
    """
    Parse to√†n b·ªô content th√†nh list c√°c c√¢u h·ªèi v·ªõi format chu·∫©n
    
    Args:
        content: To√†n b·ªô text ƒë√£ g·ªôp t·ª´ t·∫•t c·∫£ c√°c file
        page_markers: List of (start_pos, end_pos, page_name) ƒë·ªÉ track v·ªã tr√≠
    
    Returns:
        List[Dict]: M·ªói dict ch·ª©a question, options, correct_answer
    """
    
    def get_page_info(start_pos: int, end_pos: int) -> Dict:
        """X√°c ƒë·ªãnh c√¢u h·ªèi n·∫±m trong page n√†o"""
        pages = []
        for page_start, page_end, page_name in page_markers:
            # Check if question overlaps with this page
            if not (end_pos < page_start or start_pos > page_end):
                pages.append(page_name)
        
        if not pages:
            return {"primary_page": "unknown", "spans_pages": []}
        elif len(pages) == 1:
            return {"primary_page": pages[0].replace('.txt', ''), "spans_pages": []}
        else:
            return {"primary_page": pages[0].replace('.txt', ''), "spans_pages": pages}
    
    
    # Regex ƒë·ªÉ t√°ch t·ª´ng c√¢u h·ªèi v·ªõi v·ªã tr√≠
    # Pattern: C√¢u X: ... A. ... B. ... C. ... D. ... <ƒê√°p √°n: Y>
    pattern = r'C√¢u\s+(\d+)[.:]?\s*(.*?)(?=C√¢u\s+\d+[.:]?|$)'
    matches = [(m.group(1), m.group(2), m.start(), m.end()) 
               for m in re.finditer(pattern, content, re.DOTALL)]
    
    questions = []
    
    for match in matches:
        question_num = match[0]
        question_block = match[1].strip()
        start_pos = match[2]
        end_pos = match[3]
        
        # Get page info
        page_info = get_page_info(start_pos, end_pos)
        
        # T√¨m ph·∫ßn c√¢u h·ªèi (tr∆∞·ªõc c√°c l·ª±a ch·ªçn A, B, C, D)
        question_text_match = re.match(r'(.*?)(?=\s*[A-D]\.)', question_block, re.DOTALL)
        if not question_text_match:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng parse ƒë∆∞·ª£c c√¢u h·ªèi {question_num} (page: {page_info['primary_page']}) - Kh√¥ng t√¨m th·∫•y pattern c√¢u h·ªèi")
            continue
        
        question_text = question_text_match.group(1).strip()
        
        # Normalize question text
        question_text = normalize_text(question_text)
        
        # T√¨m c√°c l·ª±a ch·ªçn A, B, C, D
        options = {}
        option_pattern = r'([A-D])\.\s*(.*?)(?=\s*[A-D]\.|<ƒê√°p √°n:|$)'
        option_matches = re.findall(option_pattern, question_block, re.DOTALL)
        
        for opt_letter, opt_text in option_matches:
            # Normalize option text
            options[opt_letter] = normalize_text(opt_text)
        
        # T√¨m ƒë√°p √°n ƒë√∫ng
        answer_match = re.search(r'<ƒê√°p √°n:\s*([A-D])\s*>', question_block)
        if not answer_match:
            logger.warning(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y ƒë√°p √°n cho c√¢u {question_num} (page: {page_info['primary_page']})")
            continue
        
        correct_answer = answer_match.group(1)
        
        # Validate
        if len(options) != 4:
            logger.warning(f"‚ö†Ô∏è  C√¢u {question_num} (page: {page_info['primary_page']}) kh√¥ng ƒë·ªß 4 l·ª±a ch·ªçn (c√≥ {len(options)}: {list(options.keys())})")
            continue
        
        if correct_answer not in options:
            logger.error(f"‚ùå ƒê√°p √°n {correct_answer} kh√¥ng c√≥ trong options c√¢u {question_num} (page: {page_info['primary_page']})")
            raise ValueError(f"Invalid answer key in question {question_num}")
        
        # Create unique ID based on primary page
        question_id = f"{page_info['primary_page']}_cau_{question_num}"
        
        question_data = {
            "id": question_id,
            "question": question_text,
            "options": options,
            "correct_answer": correct_answer,
            "correct_answer_text": options[correct_answer],
            "question_number": int(question_num),
            "primary_page": page_info['primary_page'],
            "subject": "V·∫≠t l√Ω"  # Hardcode cho test
        }
        
        # Add spans_pages if question is split across pages
        if page_info['spans_pages']:
            question_data["spans_pages"] = page_info['spans_pages']
            logger.info(f"üìÑ C√¢u {question_num} b·ªã ng·∫Øt qua c√°c trang: {page_info['spans_pages']}")
        
        questions.append(question_data)
    
    return questions

# ================== MAIN PROCESS ==================
def main():
    logger.info("=" * 70)
    logger.info("B·∫ÆT ƒê·∫¶U PARSE C√ÇU H·ªéI T·ª™ TXT ‚Üí JSON")
    logger.info("=" * 70)
    
    # Check input folder
    input_path = Path(INPUT_FOLDER)
    if not input_path.exists():
        logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y folder: {INPUT_FOLDER}")
        return
    
    txt_files = sorted(input_path.glob("*.txt"))
    logger.info(f"üìÇ Folder input: {INPUT_FOLDER}")
    logger.info(f"üìÑ T√¨m th·∫•y {len(txt_files)} file txt")
    
    # Statistics
    stats = {
        "total_files": len(txt_files),
        "success_files": 0,
        "failed_files": 0,
        "total_questions": 0,
        "total_chars": 0
    }
    
    # Concatenate all files into one large text
    logger.info("üîó ƒêang g·ªôp t·∫•t c·∫£ file txt th√†nh 1 file l·ªõn...")
    full_text = ""
    page_markers = []  # Track position of each page: (start_pos, end_pos, page_name)
    
    for txt_file in tqdm(txt_files, desc="Reading files"):
        try:
            start_pos = len(full_text)
            content = txt_file.read_text(encoding='utf-8', errors='replace')
            full_text += content + "\n\n"  # Add spacing between files
            end_pos = len(full_text)
            
            page_markers.append((start_pos, end_pos, txt_file.name))
            stats["success_files"] += 1
        except Exception as e:
            logger.error(f"‚ùå L·ªói ƒë·ªçc file {txt_file.name}: {e}")
            stats["failed_files"] += 1
    
    logger.info(f"‚úì ƒê√£ g·ªôp {stats['success_files']} files")
    logger.info(f"üìè T·ªïng ƒë·ªô d√†i: {len(full_text):,} k√Ω t·ª±")
    
    # Update stats with total chars
    stats["total_chars"] = len(full_text)
    
    # Parse all questions from concatenated text
    logger.info("üîç ƒêang parse c√¢u h·ªèi t·ª´ text ƒë√£ g·ªôp...")
    
    try:
        all_questions = parse_all_questions(full_text, page_markers)
        stats["total_questions"] = len(all_questions)
        logger.info(f"‚úì ƒê√£ parse th√†nh c√¥ng {len(all_questions)} c√¢u h·ªèi")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói parse: {e}")
        logger.error("üõë D·ª™NG TO√ÄN B·ªò QU√Å TR√åNH DO L·ªñI PARSE")
        raise
    
    # Check for duplicate IDs
    ids = [q["id"] for q in all_questions]
    duplicate_ids = [id for id in ids if ids.count(id) > 1]
    
    if duplicate_ids:
        logger.warning(f"‚ö†Ô∏è  T√¨m th·∫•y {len(set(duplicate_ids))} ID tr√πng l·∫∑p!")
        for dup_id in set(duplicate_ids):
            dup_questions = [q for q in all_questions if q["id"] == dup_id]
            logger.warning(f"   - {dup_id}: Xu·∫•t hi·ªán {len(dup_questions)} l·∫ßn")
            for q in dup_questions:
                logger.warning(f"     Question #{q['question_number']}: {q['question'][:50]}...")
    
    # Count questions that span multiple pages
    split_questions = [q for q in all_questions if "spans_pages" in q]
    if split_questions:
        logger.info(f"üìÑ T√¨m th·∫•y {len(split_questions)} c√¢u h·ªèi b·ªã ng·∫Øt qua nhi·ªÅu trang")
        stats["split_questions"] = len(split_questions)
    
    # Save to JSON
    Path(OUTPUT_JSON).parent.mkdir(parents=True, exist_ok=True)
    
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(all_questions, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úì ƒê√£ l∆∞u {len(all_questions)} c√¢u h·ªèi v√†o: {OUTPUT_JSON}")
    
    # Save statistics
    stats["timestamp"] = datetime.now().isoformat()
    stats["output_file"] = OUTPUT_JSON
    
    with open(STATS_JSON, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úì ƒê√£ l∆∞u th·ªëng k√™ v√†o: {STATS_JSON}")
    
    # Print summary
    logger.info("=" * 70)
    logger.info("K·∫æT QU·∫¢ T·ªîNG K·∫æT")
    logger.info("=" * 70)
    logger.info(f"üìÅ T·ªïng s·ªë file ƒë·ªçc: {stats['total_files']}")
    logger.info(f"‚úì ƒê·ªçc th√†nh c√¥ng: {stats['success_files']}")
    logger.info(f"‚ùå ƒê·ªçc th·∫•t b·∫°i: {stats['failed_files']}")
    logger.info(f"üìè T·ªïng k√Ω t·ª±: {stats['total_chars']:,}")
    logger.info(f"üìù T·ªïng s·ªë c√¢u h·ªèi: {stats['total_questions']}")
    if duplicate_ids:
        logger.info(f"‚ö†Ô∏è  ID tr√πng l·∫∑p: {len(set(duplicate_ids))}")
    if stats.get("split_questions"):
        logger.info(f"üìÑ C√¢u h·ªèi b·ªã ng·∫Øt trang: {stats['split_questions']}")
    logger.info(f"üíæ Output JSON: {OUTPUT_JSON}")
    logger.info(f"üìä Statistics: {STATS_JSON}")
    logger.info("=" * 70)
    
    # Sample output
    logger.info("\nüìã SAMPLE - 3 c√¢u h·ªèi ƒë·∫ßu ti√™n:")
    for i, q in enumerate(all_questions[:3], 1):
        logger.info(f"\n{i}. ID: {q['id']}")
        logger.info(f"   C√¢u h·ªèi: {q['question'][:80]}...")
        logger.info(f"   ƒê√°p √°n: {q['correct_answer']} - {q['correct_answer_text'][:50]}...")
    
    logger.info("\n‚úÖ HO√ÄN T·∫§T! H√£y ki·ªÉm tra file JSON tr∆∞·ªõc khi embed.")

# ================== CLI ==================
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Ng∆∞·ªùi d√πng d·ª´ng ch∆∞∆°ng tr√¨nh")
    except Exception as e:
        logger.error(f"\n‚ùå L·ªói nghi√™m tr·ªçng: {e}", exc_info=True)